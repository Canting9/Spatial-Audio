{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Final Project Documentation for Spacial Audio Class","text":"<p>Welcome to our final project documentation website.</p> <p>This website contains:</p> <ul> <li>Project concept introduction</li> <li>Technical details and system architecture</li> <li>Documentation of the final presentation</li> <li>Images and videos</li> <li>References</li> </ul> <p>Please use the navigation bar on the top left to browse all pages.</p>"},{"location":"documentation/","title":"Documentation of the Show","text":""},{"location":"image/","title":"Images","text":""},{"location":"intro-concept/","title":"Introduction and Concept","text":""},{"location":"intro-concept/#project-overview","title":"Project Overview","text":""},{"location":"intro-concept/#artistic-motivation","title":"Artistic Motivation","text":""},{"location":"tech-details/","title":"Technical Details","text":""},{"location":"tech-details/#audio","title":"Audio","text":"<p>Our audio was running in Ableton Live 11. The session set up was split into four track groupings: Lead, Bass, Chords, Drums. Each group of tracks had four individual tracks, each housing a distinct MIDI instrument. Each group of tracks also had a OSC mapper max for live device designed to map values inside ableton based on messages coming in through OSC. These messages coming from the x-axis of the camera interaction were mapped to one unique macro parameter per track . The macro would drastically change the sonic qualities of each track and would consist of a combination of effects such as filters, sequence, distortion, redux, and delay. Each audio track was created with the goal of working sonically with each other, no matter the combination of tracks. </p>"},{"location":"tech-details/#visual","title":"Visual","text":"<p>The visual component of the project was designed to support the audio system and enhance the overall immersive environment. All visuals were developed in Jitter, combining custom Jitter patches with VIZZIE effects built on Jitter. The visual system is designed with two layers. The first layer consists of a signature particle system. Particles are triggered and modulated in real time by the audio drum volume, which is converted as OSC data.  When activated, particles appear to burst, disperse, and re-collect, flowing dynamically through virtual space correspondence with dynamic changes in the audio. The second layer functions as a background visual texture. This layer is constructed from a short, freely available visual clip featuring and pattern-based motion. The clip is heavily processed using multiple VIZZIE modules to create transformations in color, spatial distortion, feedback, and temporal delay. This background layer adds depth and variation to the visual field behind the particle system. Audience interaction is implemented with OSC control and is directly linked to the same logic used for audio interaction. Depth cameras track audience proximity and movement, and these measurements are mapped to four continuous control parameters within Jitter. These controls influence both the background visual patterns and the particle system, including parameters such as particle density, motion behavior, visual pattern, and visual shaking speed. As audiences move closer to or farther from the cameras, the visual environment responds in real time, which establishes a coherent link between physical presence, sound, and image. Overall, the visual system acts as an extension of the audio design, following the audio rhythmic structure and responding to the audience's body movements.</p>"},{"location":"tech-details/#camera-settings","title":"Camera Settings","text":"<p>For camera gesture interaction, Our system implements a real-time camera-gesture interaction pipeline using Intel RealSense depth cameras and the MediaPipe Hands model (cite). Depth sensing and RGB hand pose estimation are combined to extract two categories of interaction parameters: distance-based continuous control from the minimum depth within a predefined region of interest (ROI), and gesture-based triggers from 4 fingers recognition. The cameras stream synchronized depth and color frames. Depth frames are downsampled to reduce bandwidth and smoothed over time using a moving window average. MediaPipe Hands performs lightweight fingertip detection every N frames, making low-latency gesture recognition without using much computational resources. Each detected hand gesture information is normalized to image coordinates, and the exponential moving average (EMA) stabilizes the 3D gesture center across frames, minimizing jitter, and making hand information stable in low light and fast motion conditions. As for OSC, the component is consist of \u2018\u2019/volume\u2019\u2019, which a scalar in [0,1] mapped from the user's distance to the camera, and \u201c/1\u201d, \u201c/2\u201d,\u201d /3\u201d,\u201d /4\u201d, referring that the number of fingers map to different timbre of Lead, Bass, Chords, and Drums in audio part. Each gesture message has normalized horizontal position, vertical position, and average landmark depth. The design separates continuous and discrete control streams. That makes sure the interaction is smooth.Messages are transmitted using OSC over UDP through the \u201cpython-osc\u201dpackage. Each camera has two OSC clients: the main endpoint sent to audio and a clone endpoint sent to the visual part.</p>"},{"location":"tech-details/#interaction","title":"Interaction","text":"<p>As for Interaction implementation, when participants walk close to the camera, the large volume and strong visual expressiveness are generated. In addition, when users switch the finger numbers, the timbre of each instrument can also be switched. The 2D image coordinates (cx,cy) control parameters such as filter and azimuth of audio.</p>"},{"location":"video/","title":"Videos","text":""}]}