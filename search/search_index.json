{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Final Project Documentation for Spacial Audio Class","text":"<p>Welcome to our final project documentation website.</p> <p>This website contains:</p> <ul> <li>Project concept introduction</li> <li>Technical details and system architecture</li> <li>Documentation of the final presentation</li> <li>Images and videos</li> <li>References</li> </ul> <p>Please use the navigation bar on the top left to browse all pages.</p>"},{"location":"documentation/","title":"Documentation of the Show","text":"<p>In the actual performance, due to some unexpected factors, we were unable to control the volume using the distance detected by the camera. Furthermore, although we originally planned to use three projectors, only two were used in the actual performance. However, due to changes in the venue, two projectors were sufficient to produce excellent visual effects. Regarding the camera, we found that the gesture recognition function completely failed in a completely dark environment, so we had to keep some lights on. Overall, despite various unexpected issues, the final presentation was satisfying to the audience.</p> <p>Below are videos taken at the actual show\uff1a</p>"},{"location":"intro-concept/","title":"Introduction and Concept","text":"<p>Our installation transforms music listening into a collaborative and interactive experience. Rather than a traditional DJ setup with a single performer, our system allows multiple participants to mix, sequence, and layer different stems simultaneously, fostering spontaneity and shared creativity. Inspired by the nightlife culture of Underground Atlanta, the musical styles range from electronic and hip-hop to experimental sounds, embedding the city\u2019s underground sonic identity into the experience. The project merges interactive technology with community-driven performance, inviting participants to co-create music in an intuitive and immersive environment.</p>"},{"location":"references/","title":"References","text":"<p>Hein, E. (2021). Ableton Live 11. Journal of the American Musicological Society, 74(1), 214\u2013225. https://doi.org/10.1525/jams.2021.74.1.214. Jones, R., &amp; Nevile, B. (2005). Creating visual music in jitter: Approaches and techniques. Computer Music Journal, 29(4), 55-70. Yu, Z., &amp; Yang, D. (2020, April). Some techniques on digital signal processing in the visual programming environment Max. In 2020 3rd international conference on advanced electronic materials, computers and software engineering (AEMCSE) (pp. 584-588). IEEE. Zabatani, A., Surazhsky, V., Sperling, E., Moshe, S. B., Menashe, O., Silver, D. H., ... &amp; Kimmel, R. (2019). Intel\u00ae realsense\u2122 sr300 coded light depth camera. IEEE transactions on pattern analysis and machine intelligence, 42(10), 2333-2345. Veluri, R. K., Sree, S. R., Vanathi, A., Aparna, G., &amp; Vaidya, S. P. (2022, March). Hand gesture mapping using MediaPipe algorithm. In Proceedings of Third International Conference on Communication, Computing and Electronics Systems: ICCCES 2021 (pp. 597-614). Singapore: Springer Singapore.</p>"},{"location":"tech-details/","title":"Technical Details","text":""},{"location":"tech-details/#setup","title":"Setup","text":"<p>Set up for this project took an unexpected and unfortunate turn when the MARTA tunnel flooded. Our project presentation was pushed back a week as we braved the flood and dismantled our wet set up to rebuild it in the old RadioShack space. The process was tedious, wet, and cumbersome, but through our group and others' hard work, we were able to salvage all the speakers, trusses, and most of the cables. The rebuilding process in the new space was rolled out over a couple days of hard work as we reconfigured the set up to function within the dimensions of a smaller space. The end result still provided two layers of height for speakers around the room as well as two projectors.</p>"},{"location":"tech-details/#audio","title":"Audio","text":"<p>Our audio was running in Ableton Live 11. The session set up was split into four track groupings: Lead, Bass, Chords, Drums. Each group of tracks had four individual tracks, each housing a distinct MIDI instrument. Each group of tracks also had a OSC mapper max for live device designed to map values inside ableton based on messages coming in through OSC. These messages coming from the x-axis of the camera interaction were mapped to one unique macro parameter per track . The macro would drastically change the sonic qualities of each track and would consist of a combination of effects such as filters, sequence, distortion, redux, and delay. Each audio track was created with the goal of working sonically with each other, no matter the combination of tracks. </p> <p></p>"},{"location":"tech-details/#visual","title":"Visual","text":"<p>The visual component of the project was designed to support the audio system and enhance the overall immersive environment. All visuals were developed in Jitter, combining custom Jitter patches with VIZZIE effects built on Jitter[3]. The visual system is designed with two layers. The first layer consists of a signature particle system. Particles are triggered and modulated in real time by the audio drum volume, which is converted as OSC data.  When activated, particles appear to burst, disperse, and re-collect, flowing dynamically through virtual space correspondence with dynamic changes in the audio. The second layer functions as a background visual texture. This layer is constructed from a short, freely available visual clip featuring and pattern-based motion. The clip is heavily processed using multiple VIZZIE modules to create transformations in color, spatial distortion, feedback, and temporal delay. This background layer adds depth and variation to the visual field behind the particle system.</p> <p>Audience interaction is implemented with OSC control and is directly linked to the same logic used for audio interaction. Depth cameras track audience proximity and movement, and these measurements are mapped to four continuous control parameters within Jitter. These controls influence both the background visual patterns and the particle system, including parameters such as particle density, motion behavior, visual pattern, and visual shaking speed. As audiences move closer to or farther from the cameras, the visual environment responds in real time, which establishes a coherent link between physical presence, sound, and image. Overall, the visual system acts as an extension of the audio design, following the audio rhythmic structure and responding to the audience's body movements.</p> <p> </p>"},{"location":"tech-details/#camera-settings","title":"Camera Settings","text":"<p>For camera gesture interaction, Our system implements a real-time camera-gesture interaction pipeline using Intel RealSense depth cameras and the MediaPipe Hands model (cite). Depth sensing and RGB hand pose estimation are combined to extract two categories of interaction parameters: distance-based continuous control from the minimum depth within a predefined region of interest (ROI), and gesture-based triggers from 4 fingers recognition.</p> <p>The cameras stream synchronized depth and color frames. Depth frames are downsampled to reduce bandwidth and smoothed over time using a moving window average. MediaPipe Hands performs lightweight fingertip detection every N frames, making low-latency gesture recognition without using much computational resources. Each detected hand gesture information is normalized to image coordinates, and the exponential moving average (EMA) stabilizes the 3D gesture center across frames, minimizing jitter, and making hand information stable in low light and fast motion conditions.</p> <p>As for OSC, the component is consist of \u2018\u2019/volume\u2019\u2019, which a scalar in [0,1] mapped from the user's distance to the camera, and \u201c/1\u201d, \u201c/2\u201d,\u201d /3\u201d,\u201d /4\u201d, referring that the number of fingers map to different timbre of Lead, Bass, Chords, and Drums in audio part. Each gesture message has normalized horizontal position, vertical position, and average landmark depth. The design separates continuous and discrete control streams. That makes sure the interaction is smooth.Messages are transmitted using OSC over UDP through the \u201cpython-osc\u201dpackage. Each camera has two OSC clients: the main endpoint sent to audio and a clone endpoint sent to the visual part.</p> <p></p>"},{"location":"tech-details/#interaction","title":"Interaction","text":"<p>In terms of interactive implementation, our device supports up to four people controlling the music simultaneously. When a user faces one of the four cameras and holds up one to four fingers, each instrument plays one of four corresponding musical phrases. When the user maintains the gesture and moves their hand up, down, left, or right, various audio effects, such as filter parameters, change according to the movement coordinates, resulting in different sound experiences.</p>"}]}